{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt \n",
    "import sentencepiece as spm\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8])\n",
      "tensor([[3091, 3604,  206, 3958, 3760, 3590,    0,    0],\n",
      "        [ 212, 3605,   53, 3832, 3596, 3682, 3760, 3590]])\n"
     ]
    }
   ],
   "source": [
    "random_seed = 0\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "# 8개의 단어로 이루어진 2개의 문장이 있다고 가정\n",
    "inp = torch.tensor(\n",
    "    [[3091, 3604,  206, 3958, 3760, 3590,    0,    0],\n",
    "     [ 212, 3605,   53, 3832, 3596, 3682, 3760, 3590]], dtype=torch.long)\n",
    "print(inp.shape)\n",
    "print(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Vacab 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "겨울이 되어서 날씨가 무척 추워요.\n",
      "['▁겨울', '이', '▁되어', '서', '▁날', '씨', '가', '▁무', '척', '▁추', '워', '요', '.']\n",
      "[3198, 3538, 632, 3552, 698, 3979, 3549, 107, 4148, 197, 3906, 3710, 3540]\n",
      "\n",
      "이번 성탄절은 화이트 크리스마스가 될까요?\n",
      "['▁이번', '▁성', '탄', '절', '은', '▁화', '이트', '▁크리스', '마', '스가', '▁될', '까', '요', '?']\n",
      "[2922, 88, 3919, 3873, 3554, 268, 661, 1910, 3614, 760, 1430, 3749, 3710, 4211]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_file = \"model/kowiki.model\"\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(vocab_file)\n",
    "\n",
    "lines = [\n",
    "  \"겨울이 되어서 날씨가 무척 추워요.\",\n",
    "  \"이번 성탄절은 화이트 크리스마스가 될까요?\"\n",
    "]\n",
    "for line in lines:\n",
    "  pieces = vocab.encode_as_pieces(line)\n",
    "  ids = vocab.encode_as_ids(line)\n",
    "  print(line)\n",
    "  print(pieces)\n",
    "  print(ids)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Config 설정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict): \n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_enc_vocab': 8007, 'n_dec_vocab': 8007, 'n_enc_seq': 256, 'n_dec_seq': 256, 'n_layer': 6, 'd_hidn': 256, 'i_pad': 0, 'd_ff': 1024, 'n_head': 4, 'd_head': 64, 'dropout': 0.1, 'layer_norm_epsilon': 1e-12}\n"
     ]
    }
   ],
   "source": [
    "config = Config({\n",
    "    \"n_enc_vocab\": len(vocab),\n",
    "    \"n_dec_vocab\": len(vocab),\n",
    "    \"n_enc_seq\": 256,\n",
    "    \"n_dec_seq\": 256,\n",
    "    \"n_layer\": 6,\n",
    "    \"d_hidn\": 256,\n",
    "    \"i_pad\": 0,\n",
    "    \"d_ff\": 1024,\n",
    "    \"n_head\": 4,\n",
    "    \"d_head\": 64,\n",
    "    \"dropout\": 0.1,\n",
    "    \"layer_norm_epsilon\": 1e-12\n",
    "})\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Common Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" sinusoid position encoding \"\"\"\n",
    "def get_sinusoid_encoding_table(n_seq, d_hidn):\n",
    "    def cal_angle(position, i_hidn):\n",
    "        return position / np.power(10000, 2 * (i_hidn // 2) / d_hidn)\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i_hidn) for i_hidn in range(d_hidn)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(i_seq) for i_seq in range(n_seq)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # even index sin \n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # odd index cos\n",
    "\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" attention pad mask \"\"\"\n",
    "def get_attn_pad_mask(seq_q, seq_k, i_pad):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    pad_attn_mask = seq_k.data.eq(i_pad)\n",
    "    pad_attn_mask= pad_attn_mask.unsqueeze(1).expand(batch_size, len_q, len_k)\n",
    "    return pad_attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" attention decoder mask \"\"\"\n",
    "def get_attn_decoder_mask(seq):\n",
    "    subsequent_mask = torch.ones_like(seq).unsqueeze(-1).expand(seq.size(0), seq.size(1), seq.size(1))\n",
    "    subsequent_mask = subsequent_mask.triu(diagonal=1) # upper triangular part of a matrix(2-D)\n",
    "    return subsequent_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" scale dot product attention \"\"\"\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.scale = 1 / (self.config.d_head ** 0.5)\n",
    "    \n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # (bs, n_head, n_q_seq, n_k_seq)\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2))\n",
    "        scores = scores.mul_(self.scale)\n",
    "        scores.masked_fill_(attn_mask, -1e9)\n",
    "        # (bs, n_head, n_q_seq, n_k_seq)\n",
    "        attn_prob = nn.Softmax(dim=-1)(scores)\n",
    "        attn_prob = self.dropout(attn_prob)\n",
    "        # (bs, n_head, n_q_seq, d_v)\n",
    "        context = torch.matmul(attn_prob, V)\n",
    "        # (bs, n_head, n_q_seq, d_v), (bs, n_head, n_q_seq, n_v_seq)\n",
    "        return context, attn_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Head Attention 아키텍처"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" multi head attention \"\"\"\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.W_Q = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n",
    "        self.W_K = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n",
    "        self.W_V = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n",
    "        self.scaled_dot_attn = ScaledDotProductAttention(self.config)\n",
    "        self.linear = nn.Linear(self.config.n_head * self.config.d_head, self.config.d_hidn)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        batch_size = Q.size(0)\n",
    "        # (bs, n_head, n_q_seq, d_head)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)\n",
    "        # (bs, n_head, n_k_seq, d_head)\n",
    "        k_s = self.W_K(K).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)\n",
    "        # (bs, n_head, n_v_seq, d_head)\n",
    "        v_s = self.W_V(V).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)\n",
    "\n",
    "        # (bs, n_head, n_q_seq, n_k_seq)\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.config.n_head, 1, 1)\n",
    "\n",
    "        # (bs, n_head, n_q_seq, d_head), (bs, n_head, n_q_seq, n_k_seq)\n",
    "        context, attn_prob = self.scaled_dot_attn(q_s, k_s, v_s, attn_mask)\n",
    "        # (bs, n_head, n_q_seq, h_head * d_head)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.config.n_head * self.config.d_head)\n",
    "        # (bs, n_head, n_q_seq, e_embd)\n",
    "        output = self.linear(context)\n",
    "        output = self.dropout(output)\n",
    "        # (bs, n_q_seq, d_hidn), (bs, n_head, n_q_seq, n_k_seq)\n",
    "        return output, attn_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position-wise Feedforward 아키텍처"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" feed forward \"\"\"\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=self.config.d_hidn, out_channels=self.config.d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=self.config.d_ff, out_channels=self.config.d_hidn, kernel_size=1)\n",
    "        self.active = F.gelu\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # (bs, d_ff, n_seq)\n",
    "        output = self.conv1(inputs.transpose(1, 2))\n",
    "        output = self.active(output)\n",
    "        # (bs, n_seq, d_hidn)\n",
    "        output = self.conv2(output).transpose(1, 2)\n",
    "        output = self.dropout(output)\n",
    "        # (bs, n_seq, d_hidn)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인코더(Encoder) 레이어 아키텍처"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" encoder layer \"\"\"\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(self.config)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
    "        self.pos_ffn = PoswiseFeedForwardNet(self.config)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
    "    \n",
    "    def forward(self, inputs, attn_mask):\n",
    "        # (bs, n_enc_seq, d_hidn), (bs, n_head, n_enc_seq, n_enc_seq)\n",
    "        att_outputs, attn_prob = self.self_attn(inputs, inputs, inputs, attn_mask)\n",
    "        att_outputs = self.layer_norm1(inputs + att_outputs)\n",
    "        # (bs, n_enc_seq, d_hidn)\n",
    "        ffn_outputs = self.pos_ffn(att_outputs)\n",
    "        ffn_outputs = self.layer_norm2(ffn_outputs + att_outputs)\n",
    "        # (bs, n_enc_seq, d_hidn), (bs, n_head, n_enc_seq, n_enc_seq)\n",
    "        return ffn_outputs, attn_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인코더(Encoder) 아키텍처\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" encoder \"\"\"\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.enc_emb = nn.Embedding(self.config.n_enc_vocab, self.config.d_hidn)\n",
    "        sinusoid_table = torch.FloatTensor(get_sinusoid_encoding_table(self.config.n_enc_seq + 1, self.config.d_hidn))\n",
    "        self.pos_emb = nn.Embedding.from_pretrained(sinusoid_table, freeze=True)\n",
    "\n",
    "        self.layers = nn.ModuleList([EncoderLayer(self.config) for _ in range(self.config.n_layer)])\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        positions = torch.arange(inputs.size(1), device=inputs.device, dtype=inputs.dtype).expand(inputs.size(0), inputs.size(1)).contiguous() + 1\n",
    "        pos_mask = inputs.eq(self.config.i_pad)\n",
    "        positions.masked_fill_(pos_mask, 0)\n",
    "\n",
    "        # (bs, n_enc_seq, d_hidn)\n",
    "        outputs = self.enc_emb(inputs) + self.pos_emb(positions)\n",
    "\n",
    "        # (bs, n_enc_seq, n_enc_seq)\n",
    "        attn_mask = get_attn_pad_mask(inputs, inputs, self.config.i_pad)\n",
    "\n",
    "        attn_probs = []\n",
    "        for layer in self.layers:\n",
    "            # (bs, n_enc_seq, d_hidn), (bs, n_head, n_enc_seq, n_enc_seq)\n",
    "            outputs, attn_prob = layer(outputs, attn_mask)\n",
    "            attn_probs.append(attn_prob)\n",
    "        # (bs, n_enc_seq, d_hidn), [(bs, n_head, n_enc_seq, n_enc_seq)]\n",
    "        return outputs, attn_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 디코더(Decoder) 레이어 아키텍처"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" decoder layer \"\"\"\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(self.config)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
    "        self.dec_enc_attn = MultiHeadAttention(self.config)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
    "        self.pos_ffn = PoswiseFeedForwardNet(self.config)\n",
    "        self.layer_norm3 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
    "    \n",
    "    def forward(self, dec_inputs, enc_outputs, self_attn_mask, dec_enc_attn_mask):\n",
    "        # (bs, n_dec_seq, d_hidn), (bs, n_head, n_dec_seq, n_dec_seq)\n",
    "        self_att_outputs, self_attn_prob = self.self_attn(dec_inputs, dec_inputs, dec_inputs, self_attn_mask)\n",
    "        self_att_outputs = self.layer_norm1(dec_inputs + self_att_outputs)\n",
    "        # (bs, n_dec_seq, d_hidn), (bs, n_head, n_dec_seq, n_enc_seq)\n",
    "        dec_enc_att_outputs, dec_enc_attn_prob = self.dec_enc_attn(self_att_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\n",
    "        dec_enc_att_outputs = self.layer_norm2(self_att_outputs + dec_enc_att_outputs)\n",
    "        # (bs, n_dec_seq, d_hidn)\n",
    "        ffn_outputs = self.pos_ffn(dec_enc_att_outputs)\n",
    "        ffn_outputs = self.layer_norm3(dec_enc_att_outputs + ffn_outputs)\n",
    "        # (bs, n_dec_seq, d_hidn), (bs, n_head, n_dec_seq, n_dec_seq), (bs, n_head, n_dec_seq, n_enc_seq)\n",
    "        return ffn_outputs, self_attn_prob, dec_enc_attn_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 디코더(Decoder) 아키텍처"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" decoder \"\"\"\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.dec_emb = nn.Embedding(self.config.n_dec_vocab, self.config.d_hidn)\n",
    "        sinusoid_table = torch.FloatTensor(get_sinusoid_encoding_table(self.config.n_dec_seq + 1, self.config.d_hidn))\n",
    "        self.pos_emb = nn.Embedding.from_pretrained(sinusoid_table, freeze=True)\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(self.config) for _ in range(self.config.n_layer)])\n",
    "    \n",
    "    def forward(self, dec_inputs, enc_inputs, enc_outputs):\n",
    "        positions = torch.arange(dec_inputs.size(1), device=dec_inputs.device, dtype=dec_inputs.dtype).expand(dec_inputs.size(0), dec_inputs.size(1)).contiguous() + 1\n",
    "        pos_mask = dec_inputs.eq(self.config.i_pad)\n",
    "        positions.masked_fill_(pos_mask, 0)\n",
    "    \n",
    "        # (bs, n_dec_seq, d_hidn)\n",
    "        dec_outputs = self.dec_emb(dec_inputs) + self.pos_emb(positions)\n",
    "\n",
    "        # (bs, n_dec_seq, n_dec_seq)\n",
    "        dec_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs, self.config.i_pad)\n",
    "        # (bs, n_dec_seq, n_dec_seq)\n",
    "        dec_attn_decoder_mask = get_attn_decoder_mask(dec_inputs)\n",
    "        # (bs, n_dec_seq, n_dec_seq)\n",
    "        dec_self_attn_mask = torch.gt((dec_attn_pad_mask + dec_attn_decoder_mask), 0)\n",
    "        # (bs, n_dec_seq, n_enc_seq)\n",
    "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs, self.config.i_pad)\n",
    "\n",
    "        self_attn_probs, dec_enc_attn_probs = [], []\n",
    "        for layer in self.layers:\n",
    "            # (bs, n_dec_seq, d_hidn), (bs, n_dec_seq, n_dec_seq), (bs, n_dec_seq, n_enc_seq)\n",
    "            dec_outputs, self_attn_prob, dec_enc_attn_prob = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
    "            self_attn_probs.append(self_attn_prob)\n",
    "            dec_enc_attn_probs.append(dec_enc_attn_prob)\n",
    "        # (bs, n_dec_seq, d_hidn), [(bs, n_dec_seq, n_dec_seq)], [(bs, n_dec_seq, n_enc_seq)]S\n",
    "        return dec_outputs, self_attn_probs, dec_enc_attn_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 트랜스포머(Transformer) 아키텍처"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" transformer \"\"\"\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.encoder = Encoder(self.config)\n",
    "        self.decoder = Decoder(self.config)\n",
    "    \n",
    "    def forward(self, enc_inputs, dec_inputs):\n",
    "        # (bs, n_enc_seq, d_hidn), [(bs, n_head, n_enc_seq, n_enc_seq)]\n",
    "        enc_outputs, enc_self_attn_probs = self.encoder(enc_inputs)\n",
    "        # (bs, n_seq, d_hidn), [(bs, n_head, n_dec_seq, n_dec_seq)], [(bs, n_head, n_dec_seq, n_enc_seq)]\n",
    "        dec_outputs, dec_self_attn_probs, dec_enc_attn_probs = self.decoder(dec_inputs, enc_inputs, enc_outputs)\n",
    "        # (bs, n_dec_seq, n_dec_vocab), [(bs, n_head, n_enc_seq, n_enc_seq)], [(bs, n_head, n_dec_seq, n_dec_seq)], [(bs, n_head, n_dec_seq, n_enc_seq)]\n",
    "        return dec_outputs, enc_self_attn_probs, dec_self_attn_probs, dec_enc_attn_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" naver movie classfication \"\"\"\n",
    "class MovieClassification(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = Transformer(self.config)\n",
    "        self.projection = nn.Linear(self.config.d_hidn, self.config.n_output, bias=False)\n",
    "    \n",
    "    def forward(self, enc_inputs, dec_inputs):\n",
    "        # (bs, n_dec_seq, d_hidn), [(bs, n_head, n_enc_seq, n_enc_seq)], [(bs, n_head, n_dec_seq, n_dec_seq)], [(bs, n_head, n_dec_seq, n_enc_seq)]\n",
    "        dec_outputs, enc_self_attn_probs, dec_self_attn_probs, dec_enc_attn_probs = self.transformer(enc_inputs, dec_inputs)\n",
    "        # (bs, d_hidn)\n",
    "        dec_outputs, _ = torch.max(dec_outputs, dim=1)\n",
    "        # (bs, n_output)\n",
    "        logits = self.projection(dec_outputs)\n",
    "        # (bs, n_output), [(bs, n_head, n_enc_seq, n_enc_seq)], [(bs, n_head, n_dec_seq, n_dec_seq)], [(bs, n_head, n_dec_seq, n_enc_seq)]\n",
    "        return logits, enc_self_attn_probs, dec_self_attn_probs, dec_enc_attn_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 영화 분류 데이터셋 \"\"\"\n",
    "class MovieDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, vocab, infile):\n",
    "        self.vocab = vocab\n",
    "        self.labels = []\n",
    "        self.sentences = []\n",
    "\n",
    "        line_cnt = 0\n",
    "        with open(infile, \"r\") as f:\n",
    "            for line in f:\n",
    "                line_cnt += 1\n",
    "\n",
    "        with open(infile, \"r\") as f:\n",
    "            for i, line in enumerate(tqdm(f, total=line_cnt, desc=f\"Loading {infile}\", unit=\" lines\")):\n",
    "                data = json.loads(line)\n",
    "                self.labels.append(data[\"label\"])\n",
    "                self.sentences.append([vocab.piece_to_id(p) for p in data[\"doc\"]])\n",
    "    \n",
    "    def __len__(self):\n",
    "        assert len(self.labels) == len(self.sentences)\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return (torch.tensor(self.labels[item]),\n",
    "                torch.tensor(self.sentences[item]),\n",
    "                torch.tensor([self.vocab.piece_to_id(\"[BOS]\")]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" movie data collate_fn \"\"\"\n",
    "def movie_collate_fn(inputs):\n",
    "    labels, enc_inputs, dec_inputs = list(zip(*inputs))\n",
    "\n",
    "    enc_inputs = torch.nn.utils.rnn.pad_sequence(enc_inputs, batch_first=True, padding_value=0)\n",
    "    dec_inputs = torch.nn.utils.rnn.pad_sequence(dec_inputs, batch_first=True, padding_value=0)\n",
    "\n",
    "    batch = [\n",
    "        torch.stack(labels, dim=0),\n",
    "        enc_inputs,\n",
    "        dec_inputs,\n",
    "    ]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data/ratings_train.json: 100%|██████████| 149995/149995 [00:03<00:00, 40044.55 lines/s]\n",
      "Loading data/ratings_test.json: 100%|██████████| 49997/49997 [00:01<00:00, 35814.40 lines/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "train_dataset = MovieDataSet(vocab, \"data/ratings_train.json\")\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=movie_collate_fn)\n",
    "test_dataset = MovieDataSet(vocab, \"data/ratings_test.json\")\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=movie_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 모델 epoch 평가 \"\"\"\n",
    "def eval_epoch(config, model, data_loader):\n",
    "    matchs = []\n",
    "    model.eval()\n",
    "\n",
    "    n_word_total = 0\n",
    "    n_correct_total = 0\n",
    "    with tqdm(total=len(data_loader), desc=f\"Valid\") as pbar:\n",
    "        for i, value in enumerate(data_loader):\n",
    "            labels, enc_inputs, dec_inputs = map(lambda v: v.to(config.device), value)\n",
    "\n",
    "            outputs = model(enc_inputs, dec_inputs)\n",
    "            logits = outputs[0]\n",
    "            _, indices = logits.max(1)\n",
    "\n",
    "            match = torch.eq(indices, labels).detach()\n",
    "            matchs.extend(match.cpu())\n",
    "            accuracy = np.sum(matchs) / len(matchs) if 0 < len(matchs) else 0\n",
    "\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix_str(f\"Acc: {accuracy:.3f}\")\n",
    "    return np.sum(matchs) / len(matchs) if 0 < len(matchs) else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 모델 epoch 학습 \"\"\"\n",
    "def train_epoch(config, epoch, model, criterion, optimizer, train_loader):\n",
    "    losses = []\n",
    "    model.train()\n",
    "\n",
    "    with tqdm(total=len(train_loader), desc=f\"Train {epoch}\") as pbar:\n",
    "        for i, value in enumerate(train_loader):\n",
    "            labels, enc_inputs, dec_inputs = map(lambda v: v.to(config.device), value)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(enc_inputs, dec_inputs)\n",
    "            logits = outputs[0]\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            loss_val = loss.item()\n",
    "            losses.append(loss_val)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix_str(f\"Loss: {loss_val:.3f} ({np.mean(losses):.3f})\")\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_enc_vocab': 8007, 'n_dec_vocab': 8007, 'n_enc_seq': 256, 'n_dec_seq': 256, 'n_layer': 6, 'd_hidn': 256, 'i_pad': 0, 'd_ff': 1024, 'n_head': 4, 'd_head': 64, 'dropout': 0.1, 'layer_norm_epsilon': 1e-12, 'device': device(type='cuda'), 'n_output': 2}\n"
     ]
    }
   ],
   "source": [
    "config.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config.n_output = 2\n",
    "print(config)\n",
    "\n",
    "learning_rate = 5e-5\n",
    "n_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 0: 100%|██████████| 1172/1172 [04:47<00:00,  4.08it/s, Loss: 0.436 (0.481)]\n",
      "Valid: 100%|██████████| 391/391 [00:49<00:00,  7.85it/s, Acc: 0.798]\n",
      "Train 1: 100%|██████████| 1172/1172 [04:47<00:00,  4.08it/s, Loss: 0.334 (0.402)]\n",
      "Valid: 100%|██████████| 391/391 [00:49<00:00,  7.87it/s, Acc: 0.806]\n",
      "Train 2: 100%|██████████| 1172/1172 [04:48<00:00,  4.07it/s, Loss: 0.408 (0.376)]\n",
      "Valid: 100%|██████████| 391/391 [00:49<00:00,  7.87it/s, Acc: 0.824]\n",
      "Train 3: 100%|██████████| 1172/1172 [04:49<00:00,  4.05it/s, Loss: 0.367 (0.355)]\n",
      "Valid: 100%|██████████| 391/391 [00:49<00:00,  7.93it/s, Acc: 0.826]\n",
      "Train 4: 100%|██████████| 1172/1172 [04:50<00:00,  4.04it/s, Loss: 0.353 (0.335)]\n",
      "Valid: 100%|██████████| 391/391 [00:49<00:00,  7.88it/s, Acc: 0.827]\n",
      "Train 5: 100%|██████████| 1172/1172 [04:50<00:00,  4.03it/s, Loss: 0.303 (0.315)]\n",
      "Valid: 100%|██████████| 391/391 [00:49<00:00,  7.88it/s, Acc: 0.828]\n",
      "Train 6: 100%|██████████| 1172/1172 [04:50<00:00,  4.04it/s, Loss: 0.312 (0.295)]\n",
      "Valid: 100%|██████████| 391/391 [00:49<00:00,  7.90it/s, Acc: 0.831]\n",
      "Train 7: 100%|██████████| 1172/1172 [04:51<00:00,  4.03it/s, Loss: 0.176 (0.271)]\n",
      "Valid: 100%|██████████| 391/391 [00:49<00:00,  7.87it/s, Acc: 0.832]\n",
      "Train 8: 100%|██████████| 1172/1172 [04:50<00:00,  4.04it/s, Loss: 0.215 (0.249)]\n",
      "Valid: 100%|██████████| 391/391 [00:49<00:00,  7.88it/s, Acc: 0.831]\n",
      "Train 9: 100%|██████████| 1172/1172 [04:51<00:00,  4.02it/s, Loss: 0.292 (0.225)]\n",
      "Valid: 100%|██████████| 391/391 [00:49<00:00,  7.86it/s, Acc: 0.831]\n"
     ]
    }
   ],
   "source": [
    "model = MovieClassification(config)\n",
    "model.to(config.device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "losses, scores = [], []\n",
    "for epoch in range(n_epoch):\n",
    "    loss = train_epoch(config, epoch, model, criterion, optimizer, train_loader)\n",
    "    score = eval_epoch(config, model, test_loader)\n",
    "\n",
    "    losses.append(loss)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.480647</td>\n",
       "      <td>0.798408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.402274</td>\n",
       "      <td>0.805668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.376106</td>\n",
       "      <td>0.824429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.355466</td>\n",
       "      <td>0.826310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.334961</td>\n",
       "      <td>0.826730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.314647</td>\n",
       "      <td>0.828250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.294807</td>\n",
       "      <td>0.830570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.270875</td>\n",
       "      <td>0.831770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.248941</td>\n",
       "      <td>0.831130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.225315</td>\n",
       "      <td>0.830810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss     score\n",
       "0  0.480647  0.798408\n",
       "1  0.402274  0.805668\n",
       "2  0.376106  0.824429\n",
       "3  0.355466  0.826310\n",
       "4  0.334961  0.826730\n",
       "5  0.314647  0.828250\n",
       "6  0.294807  0.830570\n",
       "7  0.270875  0.831770\n",
       "8  0.248941  0.831130\n",
       "9  0.225315  0.830810"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# table\n",
    "data = {\n",
    "    \"loss\": losses,\n",
    "    \"score\": scores\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "display(df)\n",
    "\n",
    "# graph\n",
    "plt.figure(figsize=[12, 4])\n",
    "plt.plot(losses, label=\"loss\")\n",
    "plt.plot(scores, label=\"score\")\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8e5e08de07fba0fb72df28d1d02d73caaafb877a45fd93aa5b25ed7e0e8c6fec"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('torch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
